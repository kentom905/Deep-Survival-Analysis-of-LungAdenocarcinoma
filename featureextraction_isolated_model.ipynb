{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "featureextraction_isolated_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV6y0vtWuWQZ",
        "outputId": "345cc7b4-b7d1-4940-8353-82605565c018"
      },
      "source": [
        "''' Imports and loss function definitions  '''\n",
        "!pip install h5py\n",
        "!pip install lifelines\n",
        "import h5py\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from lifelines.utils import concordance_index\n",
        "import collections\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def nll_loss(hazards, S, Y, c, alpha=0.5, eps=1e-7):\n",
        "    batch_size = len(Y)\n",
        "    Y = Y.view(batch_size, 1) \n",
        "    c = c.view(batch_size, 1).float() \n",
        "    if S is None:\n",
        "        S = torch.cumprod(1 - hazards, dim=1)\n",
        "    S_padded = torch.cat([torch.ones_like(c), S], 1) \n",
        "    uncensored_loss = -(c) * (torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps)) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))\n",
        "    censored_loss = - (1-c) * torch.log(torch.gather(S_padded, 1, Y+1).clamp(min=eps))\n",
        "    neg_l = censored_loss + uncensored_loss\n",
        "    loss = (1-alpha) * neg_l + alpha * uncensored_loss\n",
        "    loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "def ce_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):\n",
        "    batch_size = len(Y)\n",
        "    Y = Y.view(batch_size, 1) \n",
        "    c = c.view(batch_size, 1).float() \n",
        "    if S is None:\n",
        "        S = torch.cumprod(1 - hazards, dim=1) \n",
        "    S_padded = torch.cat([torch.ones_like(c), S], 1)\n",
        "    reg = -(c) * (torch.log(torch.gather(S_padded, 1, Y)+eps) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))\n",
        "    ce_l = - (1-c) * torch.log(torch.gather(S, 1, Y).clamp(min=eps)) - (c) * torch.log(1 - torch.gather(S, 1, Y).clamp(min=eps))\n",
        "    loss = (1-alpha) * ce_l + alpha * reg\n",
        "    loss = loss.mean()\n",
        "    return loss\n",
        "\n",
        "class CrossEntropySurvLoss(object):\n",
        "    def __init__(self, alpha=0.15):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, hazards, S, Y, c, alpha=None): \n",
        "        if alpha is None:\n",
        "            return ce_loss(hazards, S, Y, c, alpha=self.alpha)\n",
        "        else:\n",
        "            return ce_loss(hazards, S, Y, c, alpha=alpha)\n",
        "\n",
        "class NLLSurvLoss(object):\n",
        "    def __init__(self, alpha=0.5):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, hazards, S, Y, c, alpha=None):\n",
        "        if alpha is None:\n",
        "            return nll_loss(hazards, S, Y, c, alpha=self.alpha)\n",
        "        else:\n",
        "            return nll_loss(hazards, S, Y, c, alpha=alpha)\n",
        "\n",
        "class CoxSurvLoss(object):\n",
        "    def __call__(self,hazards, S, c, **kwargs):\n",
        "        current_batch_len = len(S)\n",
        "        R_mat = np.zeros([current_batch_len, current_batch_len], dtype=int)\n",
        "        for i in range(current_batch_len):\n",
        "            for j in range(current_batch_len):\n",
        "                R_mat[i,j] = S[0][j] >= S[0][i]\n",
        "    \n",
        "        R_mat = torch.FloatTensor(R_mat).to(device)\n",
        "        theta = hazards.reshape(-1)\n",
        "        exp_theta = torch.exp(theta)\n",
        "        loss_cox = -torch.mean((theta - torch.log(torch.sum(exp_theta*R_mat, dim=1))) * (c))\n",
        "        return loss_cox\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.7/dist-packages (0.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: formulaic<0.3,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.2.4)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: interface-meta>=1.2 in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.2.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.13.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->lifelines) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3i4Tohaucl-",
        "outputId": "37a75abb-b240-4ec5-e254-ea6c58c309e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyR8FK0HuiwE"
      },
      "source": [
        "''' Attention Model definition suited for extracting survival/hazard rates '''\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.L = 1024\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(20, 50, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(50 * 53 * 53, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, 4),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, **kwargs):\n",
        "        H = kwargs['x']\n",
        "        A= self.attention(H)  \n",
        "        A = torch.transpose(A, 1, 0) \n",
        "        A_raw = A \n",
        "        A = F.softmax(A, dim=1)\n",
        "        M = torch.mm(A, H) \n",
        "        logits  = self.classifier(M) \n",
        "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
        "        hazards = torch.sigmoid(logits)\n",
        "        S = torch.cumprod(1 - hazards, dim=1)\n",
        "\n",
        "        \n",
        "        return hazards, S, Y_hat, A_raw\n",
        "\n",
        "\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGQB7XtevCzA"
      },
      "source": [
        "''' Input to model taken from filtered clinical dataset to include WSI patch, event observation and event duration. \n",
        "    It is to be noted that although the variable for event observation is defined as \"censor\" it contains value of event observation.\n",
        "    This particular definition is for the training phase '''\n",
        "class InputData(torch.utils.data.Dataset):\n",
        "    def __init__(self,train_image, test_image, valid_image,censor='/content/drive/MyDrive/propro/train_valid_censor_1.pt',event_time='/content/drive/MyDrive/propro/train_valid_et_1.pt' ,mode = 'train', transform = None):\n",
        "        self.censor=censor\n",
        "        self.event_time=event_time\n",
        "        self.mode = mode\n",
        "        self.final_censor = torch.load(self.censor)\n",
        "        self.final_event_time = torch.load(self.event_time)\n",
        "        self.train_image = train_image\n",
        "        self.test_image = test_image\n",
        "        self.valid_image = valid_image\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()]) \n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return 179709\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        temp_ind=index\n",
        "        if temp_ind>=0 and temp_ind<146926:\n",
        "          self.image=self.train_image\n",
        "          ind=temp_ind\n",
        "        elif temp_ind>=146926 and temp_ind<179709:\n",
        "          self.image=self.valid_image\n",
        "          ind=temp_ind-146926\n",
        "        img_test = self.image[ind]\n",
        "        input_batch = img_test\n",
        "        censor = self.final_censor[index]\n",
        "        event_time = self.final_event_time[index]\n",
        "        \n",
        "        \n",
        "        \n",
        "        return (input_batch, censor,event_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ93yet5vI4c"
      },
      "source": [
        "''' Input to model taken from filtered clinical dataset to include WSI patch, event observation and event duration. \n",
        "    It is to be noted that although the variable for event observation is defined as \"censor\" it contains value of event observation.\n",
        "    This particular definition is for the testing/validation phase '''\n",
        "class InputTest(torch.utils.data.Dataset):\n",
        "    def __init__(self,image,censor='/content/drive/MyDrive/propro/test_censor_1.pt',event_time='/content/drive/MyDrive/propro/test_et_1.pt' ,mode = 'test', transform = None):\n",
        "        self.censor=censor\n",
        "        self.event_time=event_time\n",
        "        self.mode = mode\n",
        "        self.final_censor = torch.load(self.censor)\n",
        "        self.final_event_time = torch.load(self.event_time)\n",
        "        self.dset = image\n",
        "           \n",
        "    def __len__(self):\n",
        "        return len(self.dset)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_test =self.dset[index]\n",
        "        input_batch = img_test\n",
        "        censor = self.final_censor[index]\n",
        "        event_time = self.final_event_time[index]\n",
        "        \n",
        "        \n",
        "        \n",
        "        return (input_batch, censor,event_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSoRF_RkvL9S"
      },
      "source": [
        "''' Training is guided by a loss function that takes in survival information and optimizer updates after every 16 slides  '''\n",
        "def train_loop_survival(epoch, model, loader, optimizer, loss_fn=NLLSurvLoss(),gc=16):   \n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "    model.train()\n",
        "    train_loss_surv = 0.\n",
        "\n",
        "    print('\\n')\n",
        "    risk_scores = np.zeros((len(loader)))\n",
        "    observations = np.zeros((len(loader)))\n",
        "    event_times = np.zeros((len(loader)))\n",
        "\n",
        "    for batch_idx, (data_WSI,censor,event_time) in enumerate(loader):\n",
        "        if censor.item() == -1.0:\n",
        "            continue\n",
        "        model.train()\n",
        "        data_WSI= data_WSI.to(device)    \n",
        "        hazards, S, Y_hat, _ = model(x=data_WSI)       \n",
        "        c=censor.to(device)\n",
        "        loss = loss_fn(hazards,S,Y_hat,c)\n",
        "        loss_value = loss.item()\n",
        "        risk = -torch.sum(S, dim=1).detach().cpu().numpy()\n",
        "        risk_scores[batch_idx] = risk\n",
        "        observations[batch_idx] = c.item()\n",
        "        event_times[batch_idx] = event_time       \n",
        "\n",
        "        train_loss_surv += loss_value\n",
        "  \n",
        "        if (batch_idx + 1) % 10000 == 0:\n",
        "            print('batch {}, loss: {:.4f}, risk: {:.4f}, bag_size: {}'.format(batch_idx, loss_value,float(risk), data_WSI.size(0)))\n",
        "        loss = loss / gc \n",
        "        loss.backward()\n",
        "\n",
        "        if (batch_idx + 1) % gc == 0: \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    train_loss_surv /= len(loader)\n",
        "    c_index = concordance_index(event_times, risk_scores, event_observed=observations)\n",
        "    print('Epoch: {}, train_loss_surv: {:.4f}, train_c_index: {:.4f}'.format(epoch, train_loss_surv, c_index))\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3rD938YvPgq"
      },
      "source": [
        "''' Used for validation/testing '''\n",
        "def validate_survival(epoch, model, loader, loss_fn=NLLSurvLoss()):\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    val_loss_surv= 0.\n",
        "    risk_scores = np.zeros((len(loader)))\n",
        "    observations = np.zeros((len(loader)))\n",
        "    event_times = np.zeros((len(loader)))   \n",
        "    for batch_idx, (data_WSI,censor,event_time) in enumerate(loader):\n",
        "        if censor.item() == -1.0:\n",
        "            continue\n",
        "        data_WSI = data_WSI.to(device)\n",
        "        c = censor.to(device)\n",
        "        with torch.no_grad():\n",
        "            hazards, S, Y_hat, _ = model(x=data_WSI)\n",
        "\n",
        "        loss = loss_fn(hazards,S,Y_hat,c)\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        risk = -torch.sum(S, dim=1).cpu().numpy()\n",
        "        risk_scores[batch_idx] = risk\n",
        "        observations[batch_idx] = c.cpu().numpy()\n",
        "        event_times[batch_idx] = event_time\n",
        "\n",
        "        val_loss_surv += loss_value\n",
        "        \n",
        "    val_loss_surv /= len(loader)\n",
        "    \n",
        "    c_index = concordance_index(event_times, risk_scores, event_observed=observations)\n",
        "    \n",
        "    print('Epoch: {}, val_loss_surv: {:.4f}, val_c_index: {:.4f}'.format(epoch, val_loss_surv, c_index))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp0_Jn6-vbpo"
      },
      "source": [
        "''' Input patches and unique ID stored in variables '''\n",
        "import h5py\n",
        "train = \"/content/drive/MyDrive/propro/hdf5_TCGAFFPE_LUAD_5x_perP_he_train.h5\"\n",
        "test = \"/content/drive/MyDrive/propro/hdf5_TCGAFFPE_LUAD_5x_perP_he_test.h5\"\n",
        "valid = \"/content/drive/MyDrive/propro/hdf5_TCGAFFPE_LUAD_5x_perP_he_validation.h5\"\n",
        "hdf5_train = h5py.File(train, \"r\")\n",
        "hdf5_test = h5py.File(test, \"r\")\n",
        "hdf5_valid = h5py.File(valid, \"r\")\n",
        "#dtrain = hdf5_train['train_img']\n",
        "#dtest = hdf5_test['test_img']\n",
        "#dvalid = hdf5_valid['valid_img']\n",
        "train_slides = hdf5_train['train_slides']\n",
        "test_slides = hdf5_test['test_slides']\n",
        "valid_slides = hdf5_valid['valid_slides']\n",
        "dtrain=torch.load('/content/drive/MyDrive/propro/train__resnet50_pretrained_image_features_dec2.pt')\n",
        "dtest=torch.load('/content/drive/MyDrive/propro/test__resnet50_pretrained_image_features_dec2.pt')\n",
        "dvalid=torch.load('/content/drive/MyDrive/propro/valid__resnet50_pretrained_image_features_dec2.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slPjfVpjvsxT",
        "outputId": "28e04fea-a7a4-46c0-a208-680e28d3ceb8"
      },
      "source": [
        "''' Model Training phase '''\n",
        "print('Initializing the Model')\n",
        "model = Attention()\n",
        "model.cuda()\n",
        "print('\\nInitializing the optimizer ')\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "epoch = 10\n",
        "\n",
        "for i in range(epoch):\n",
        "    print(\"...Train...\")\n",
        "    for x in range(1):\n",
        "      train_data = InputData(train_image=dtrain,test_image=dtest,valid_image=dvalid)   \n",
        "      test_data = InputTest(image=dtest)\n",
        "      train_loader = torch.utils.data.DataLoader(train_data, shuffle = True, num_workers = 0, batch_size = 1)\n",
        "      test_loader = torch.utils.data.DataLoader(test_data, shuffle = False, num_workers = 0, batch_size = 1)\n",
        "      train_loop_survival(i,model,train_loader,optimizer)\n",
        "      validate_survival(1,model,test_loader)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the Model\n",
            "\n",
            "Initializing the optimizer \n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.8847, risk: -0.8521, bag_size: 1\n",
            "batch 19999, loss: 1.1931, risk: -0.8647, bag_size: 1\n",
            "batch 29999, loss: 1.8546, risk: -0.8726, bag_size: 1\n",
            "batch 39999, loss: 1.8690, risk: -0.8807, bag_size: 1\n",
            "batch 49999, loss: 1.8836, risk: -0.8847, bag_size: 1\n",
            "batch 59999, loss: 1.1527, risk: -0.8895, bag_size: 1\n",
            "batch 69999, loss: 1.5419, risk: -0.9179, bag_size: 1\n",
            "batch 79999, loss: 2.5534, risk: -0.9201, bag_size: 1\n",
            "batch 89999, loss: 2.5663, risk: -0.9218, bag_size: 1\n",
            "batch 99999, loss: 2.5130, risk: -0.9185, bag_size: 1\n",
            "batch 109999, loss: 1.5367, risk: -0.9207, bag_size: 1\n",
            "batch 119999, loss: 1.4962, risk: -0.9248, bag_size: 1\n",
            "batch 129999, loss: 2.5894, risk: -0.9244, bag_size: 1\n",
            "batch 139999, loss: 1.5423, risk: -0.9205, bag_size: 1\n",
            "batch 149999, loss: 1.5018, risk: -0.9244, bag_size: 1\n",
            "batch 159999, loss: 1.5257, risk: -0.9222, bag_size: 1\n",
            "batch 169999, loss: 2.5972, risk: -0.9254, bag_size: 1\n",
            "Epoch: 0, train_loss_surv: 1.8178, train_c_index: 0.4845\n",
            "Epoch: 1, val_loss_surv: 1.8866, val_c_index: 0.4907\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.5269, risk: -0.9221, bag_size: 1\n",
            "batch 19999, loss: 2.5669, risk: -0.9231, bag_size: 1\n",
            "batch 29999, loss: 2.5928, risk: -0.9251, bag_size: 1\n",
            "batch 39999, loss: 1.4928, risk: -0.9254, bag_size: 1\n",
            "batch 49999, loss: 2.6324, risk: -0.9279, bag_size: 1\n",
            "batch 59999, loss: 1.4836, risk: -0.9263, bag_size: 1\n",
            "batch 69999, loss: 1.5178, risk: -0.9230, bag_size: 1\n",
            "batch 79999, loss: 1.4426, risk: -0.9308, bag_size: 1\n",
            "batch 89999, loss: 1.4770, risk: -0.9271, bag_size: 1\n",
            "batch 99999, loss: 1.4652, risk: -0.9283, bag_size: 1\n",
            "batch 109999, loss: 1.4815, risk: -0.9266, bag_size: 1\n",
            "batch 119999, loss: 1.4915, risk: -0.9256, bag_size: 1\n",
            "batch 129999, loss: 2.5261, risk: -0.9200, bag_size: 1\n",
            "batch 139999, loss: 1.4811, risk: -0.9267, bag_size: 1\n",
            "batch 149999, loss: 1.4851, risk: -0.9262, bag_size: 1\n",
            "batch 159999, loss: 1.4717, risk: -0.9276, bag_size: 1\n",
            "batch 169999, loss: 1.5072, risk: -0.9240, bag_size: 1\n",
            "Epoch: 1, train_loss_surv: 1.9718, train_c_index: 0.4638\n",
            "Epoch: 1, val_loss_surv: 1.8825, val_c_index: 0.4939\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.6446, risk: -0.9289, bag_size: 1\n",
            "batch 19999, loss: 2.5688, risk: -0.9233, bag_size: 1\n",
            "batch 29999, loss: 1.5145, risk: -0.9233, bag_size: 1\n",
            "batch 39999, loss: 2.6559, risk: -0.9297, bag_size: 1\n",
            "batch 49999, loss: 1.4683, risk: -0.9280, bag_size: 1\n",
            "batch 59999, loss: 2.6534, risk: -0.9295, bag_size: 1\n",
            "batch 69999, loss: 1.5244, risk: -0.9224, bag_size: 1\n",
            "batch 79999, loss: 1.4690, risk: -0.9279, bag_size: 1\n",
            "batch 89999, loss: 1.5191, risk: -0.9229, bag_size: 1\n",
            "batch 99999, loss: 2.5387, risk: -0.9210, bag_size: 1\n",
            "batch 109999, loss: 1.4332, risk: -0.9318, bag_size: 1\n",
            "batch 119999, loss: 2.6032, risk: -0.9259, bag_size: 1\n",
            "batch 129999, loss: 2.6513, risk: -0.9294, bag_size: 1\n",
            "batch 139999, loss: 1.5725, risk: -0.9180, bag_size: 1\n",
            "batch 149999, loss: 2.5613, risk: -0.9227, bag_size: 1\n",
            "batch 159999, loss: 1.5953, risk: -0.9161, bag_size: 1\n",
            "batch 169999, loss: 1.4554, risk: -0.9294, bag_size: 1\n",
            "Epoch: 2, train_loss_surv: 1.9691, train_c_index: 0.4575\n",
            "Epoch: 1, val_loss_surv: 1.8812, val_c_index: 0.4972\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.5652, risk: -0.9230, bag_size: 1\n",
            "batch 19999, loss: 2.5175, risk: -0.9192, bag_size: 1\n",
            "batch 29999, loss: 2.5048, risk: -0.9182, bag_size: 1\n",
            "batch 39999, loss: 2.6096, risk: -0.9264, bag_size: 1\n",
            "batch 49999, loss: 2.6457, risk: -0.9290, bag_size: 1\n",
            "batch 59999, loss: 2.5139, risk: -0.9190, bag_size: 1\n",
            "batch 69999, loss: 2.5923, risk: -0.9251, bag_size: 1\n",
            "batch 79999, loss: 1.5221, risk: -0.9226, bag_size: 1\n",
            "batch 89999, loss: 2.5235, risk: -0.9197, bag_size: 1\n",
            "batch 99999, loss: 1.4908, risk: -0.9257, bag_size: 1\n",
            "batch 109999, loss: 1.4484, risk: -0.9302, bag_size: 1\n",
            "batch 119999, loss: 2.6587, risk: -0.9299, bag_size: 1\n",
            "batch 129999, loss: 1.5578, risk: -0.9193, bag_size: 1\n",
            "batch 139999, loss: 2.4223, risk: -0.9112, bag_size: 1\n",
            "batch 149999, loss: 2.5509, risk: -0.9219, bag_size: 1\n",
            "batch 159999, loss: 2.4155, risk: -0.9106, bag_size: 1\n",
            "batch 169999, loss: 1.4397, risk: -0.9311, bag_size: 1\n",
            "Epoch: 3, train_loss_surv: 1.9673, train_c_index: 0.4510\n",
            "Epoch: 1, val_loss_surv: 1.8811, val_c_index: 0.4997\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.6323, risk: -0.9280, bag_size: 1\n",
            "batch 19999, loss: 1.5964, risk: -0.9160, bag_size: 1\n",
            "batch 29999, loss: 2.4345, risk: -0.9123, bag_size: 1\n",
            "batch 39999, loss: 1.4627, risk: -0.9286, bag_size: 1\n",
            "batch 49999, loss: 2.5300, risk: -0.9203, bag_size: 1\n",
            "batch 59999, loss: 1.5153, risk: -0.9232, bag_size: 1\n",
            "batch 69999, loss: 1.5643, risk: -0.9187, bag_size: 1\n",
            "batch 79999, loss: 1.6419, risk: -0.9124, bag_size: 1\n",
            "batch 89999, loss: 1.4860, risk: -0.9262, bag_size: 1\n",
            "batch 99999, loss: 2.5369, risk: -0.9208, bag_size: 1\n",
            "batch 109999, loss: 1.4404, risk: -0.9311, bag_size: 1\n",
            "batch 119999, loss: 2.4528, risk: -0.9139, bag_size: 1\n",
            "batch 129999, loss: 1.4977, risk: -0.9250, bag_size: 1\n",
            "batch 139999, loss: 2.4639, risk: -0.9149, bag_size: 1\n",
            "batch 149999, loss: 1.5307, risk: -0.9218, bag_size: 1\n",
            "batch 159999, loss: 1.4118, risk: -0.9343, bag_size: 1\n",
            "batch 169999, loss: 1.4454, risk: -0.9305, bag_size: 1\n",
            "Epoch: 4, train_loss_surv: 1.9659, train_c_index: 0.4456\n",
            "Epoch: 1, val_loss_surv: 1.8797, val_c_index: 0.5024\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.4306, risk: -0.9120, bag_size: 1\n",
            "batch 19999, loss: 1.6682, risk: -0.9105, bag_size: 1\n",
            "batch 29999, loss: 2.6105, risk: -0.9264, bag_size: 1\n",
            "batch 39999, loss: 1.5162, risk: -0.9231, bag_size: 1\n",
            "batch 49999, loss: 1.5446, risk: -0.9205, bag_size: 1\n",
            "batch 59999, loss: 1.5092, risk: -0.9238, bag_size: 1\n",
            "batch 69999, loss: 1.4832, risk: -0.9264, bag_size: 1\n",
            "batch 79999, loss: 2.5540, risk: -0.9222, bag_size: 1\n",
            "batch 89999, loss: 1.4512, risk: -0.9298, bag_size: 1\n",
            "batch 99999, loss: 1.4713, risk: -0.9277, bag_size: 1\n",
            "batch 109999, loss: 1.4526, risk: -0.9297, bag_size: 1\n",
            "batch 119999, loss: 2.6686, risk: -0.9306, bag_size: 1\n",
            "batch 129999, loss: 1.6284, risk: -0.9135, bag_size: 1\n",
            "batch 139999, loss: 2.6729, risk: -0.9309, bag_size: 1\n",
            "batch 149999, loss: 2.4255, risk: -0.9115, bag_size: 1\n",
            "batch 159999, loss: 2.4362, risk: -0.9125, bag_size: 1\n",
            "batch 169999, loss: 2.7127, risk: -0.9336, bag_size: 1\n",
            "Epoch: 5, train_loss_surv: 1.9648, train_c_index: 0.4412\n",
            "Epoch: 1, val_loss_surv: 1.8803, val_c_index: 0.5052\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.5019, risk: -0.9180, bag_size: 1\n",
            "batch 19999, loss: 2.5375, risk: -0.9209, bag_size: 1\n",
            "batch 29999, loss: 1.6734, risk: -0.9101, bag_size: 1\n",
            "batch 39999, loss: 2.4543, risk: -0.9139, bag_size: 1\n",
            "batch 49999, loss: 1.5750, risk: -0.9178, bag_size: 1\n",
            "batch 59999, loss: 1.4213, risk: -0.9332, bag_size: 1\n",
            "batch 69999, loss: 1.6493, risk: -0.9119, bag_size: 1\n",
            "batch 79999, loss: 1.5426, risk: -0.9206, bag_size: 1\n",
            "batch 89999, loss: 1.6428, risk: -0.9124, bag_size: 1\n",
            "batch 99999, loss: 2.5457, risk: -0.9214, bag_size: 1\n",
            "batch 109999, loss: 2.6314, risk: -0.9280, bag_size: 1\n",
            "batch 119999, loss: 2.6978, risk: -0.9326, bag_size: 1\n",
            "batch 129999, loss: 1.4813, risk: -0.9266, bag_size: 1\n",
            "batch 139999, loss: 2.4484, risk: -0.9135, bag_size: 1\n",
            "batch 149999, loss: 1.4684, risk: -0.9280, bag_size: 1\n",
            "batch 159999, loss: 1.5146, risk: -0.9233, bag_size: 1\n",
            "batch 169999, loss: 1.4664, risk: -0.9282, bag_size: 1\n",
            "Epoch: 6, train_loss_surv: 1.9638, train_c_index: 0.4372\n",
            "Epoch: 1, val_loss_surv: 1.8788, val_c_index: 0.5083\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.5446, risk: -0.9205, bag_size: 1\n",
            "batch 19999, loss: 2.4919, risk: -0.9172, bag_size: 1\n",
            "batch 29999, loss: 1.6078, risk: -0.9151, bag_size: 1\n",
            "batch 39999, loss: 1.5103, risk: -0.9237, bag_size: 1\n",
            "batch 49999, loss: 1.5054, risk: -0.9242, bag_size: 1\n",
            "batch 59999, loss: 1.4949, risk: -0.9253, bag_size: 1\n",
            "batch 69999, loss: 2.6250, risk: -0.9275, bag_size: 1\n",
            "batch 79999, loss: 1.4965, risk: -0.9251, bag_size: 1\n",
            "batch 89999, loss: 2.7197, risk: -0.9340, bag_size: 1\n",
            "batch 99999, loss: 2.5502, risk: -0.9219, bag_size: 1\n",
            "batch 109999, loss: 1.5227, risk: -0.9225, bag_size: 1\n",
            "batch 119999, loss: 2.4679, risk: -0.9152, bag_size: 1\n",
            "batch 129999, loss: 2.4534, risk: -0.9139, bag_size: 1\n",
            "batch 139999, loss: 2.4003, risk: -0.9093, bag_size: 1\n",
            "batch 149999, loss: 1.5186, risk: -0.9229, bag_size: 1\n",
            "batch 159999, loss: 2.5566, risk: -0.9224, bag_size: 1\n",
            "batch 169999, loss: 1.5772, risk: -0.9176, bag_size: 1\n",
            "Epoch: 7, train_loss_surv: 1.9631, train_c_index: 0.4331\n",
            "Epoch: 1, val_loss_surv: 1.8787, val_c_index: 0.5110\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.4794, risk: -0.9268, bag_size: 1\n",
            "batch 19999, loss: 1.5662, risk: -0.9186, bag_size: 1\n",
            "batch 29999, loss: 2.5385, risk: -0.9209, bag_size: 1\n",
            "batch 39999, loss: 2.4361, risk: -0.9125, bag_size: 1\n",
            "batch 49999, loss: 1.5619, risk: -0.9189, bag_size: 1\n",
            "batch 59999, loss: 1.5843, risk: -0.9170, bag_size: 1\n",
            "batch 69999, loss: 1.6295, risk: -0.9134, bag_size: 1\n",
            "batch 79999, loss: 2.5632, risk: -0.9228, bag_size: 1\n",
            "batch 89999, loss: 1.5137, risk: -0.9234, bag_size: 1\n",
            "batch 99999, loss: 2.6715, risk: -0.9308, bag_size: 1\n",
            "batch 109999, loss: 2.6542, risk: -0.9296, bag_size: 1\n",
            "batch 119999, loss: 1.4753, risk: -0.9273, bag_size: 1\n",
            "batch 129999, loss: 1.4210, risk: -0.9332, bag_size: 1\n",
            "batch 139999, loss: 1.5735, risk: -0.9179, bag_size: 1\n",
            "batch 149999, loss: 2.4252, risk: -0.9115, bag_size: 1\n",
            "batch 159999, loss: 1.5614, risk: -0.9190, bag_size: 1\n",
            "batch 169999, loss: 2.6346, risk: -0.9282, bag_size: 1\n",
            "Epoch: 8, train_loss_surv: 1.9624, train_c_index: 0.4306\n",
            "Epoch: 1, val_loss_surv: 1.8791, val_c_index: 0.5138\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.4403, risk: -0.9128, bag_size: 1\n",
            "batch 19999, loss: 2.4520, risk: -0.9138, bag_size: 1\n",
            "batch 29999, loss: 2.5129, risk: -0.9189, bag_size: 1\n",
            "batch 39999, loss: 2.4360, risk: -0.9123, bag_size: 1\n",
            "batch 49999, loss: 1.5838, risk: -0.9170, bag_size: 1\n",
            "batch 59999, loss: 1.4226, risk: -0.9331, bag_size: 1\n",
            "batch 69999, loss: 2.7291, risk: -0.9347, bag_size: 1\n",
            "batch 79999, loss: 2.5876, risk: -0.9247, bag_size: 1\n",
            "batch 89999, loss: 1.5700, risk: -0.9182, bag_size: 1\n",
            "batch 99999, loss: 1.4465, risk: -0.9304, bag_size: 1\n",
            "batch 109999, loss: 2.6225, risk: -0.9273, bag_size: 1\n",
            "batch 119999, loss: 2.6439, risk: -0.9289, bag_size: 1\n",
            "batch 129999, loss: 1.5979, risk: -0.9159, bag_size: 1\n",
            "batch 139999, loss: 2.5451, risk: -0.9215, bag_size: 1\n",
            "batch 149999, loss: 2.4958, risk: -0.9175, bag_size: 1\n",
            "batch 159999, loss: 2.4125, risk: -0.9104, bag_size: 1\n",
            "batch 169999, loss: 1.5529, risk: -0.9197, bag_size: 1\n",
            "Epoch: 9, train_loss_surv: 1.9618, train_c_index: 0.4279\n",
            "Epoch: 1, val_loss_surv: 1.8782, val_c_index: 0.5167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2zv3aA1vv85",
        "outputId": "24827fe7-75b7-495e-d87f-12c8d777f560"
      },
      "source": [
        "''' Continue model training from saved parameters '''\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/propro/resnet50_nllloss_epoch40_05536.pt'))\n",
        "print('\\nInit optimizer ...', end=' ')\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4, weight_decay=1e-5)\n",
        "model.train()\n",
        "epoch = 10 #4\n",
        "\n",
        "for i in range(epoch):\n",
        "    print(\"...Train...\")\n",
        "    for x in range(1):\n",
        "      train_data = InputData(train_image=dtrain,test_image=dtest,valid_image=dvalid)   \n",
        "      test_data = InputTest(image=dtest)\n",
        "      train_loader = torch.utils.data.DataLoader(train_data, shuffle = True, num_workers = 0, batch_size = 1)\n",
        "      test_loader = torch.utils.data.DataLoader(test_data, shuffle = False, num_workers = 0, batch_size = 1)\n",
        "      train_loop_survival(i,model,train_loader,optimizer)\n",
        "      validate_survival(1,model,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Init optimizer ... ...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.4148, risk: -0.9340, bag_size: 1\n",
            "batch 19999, loss: 1.3997, risk: -0.9358, bag_size: 1\n",
            "batch 29999, loss: 1.5918, risk: -0.9164, bag_size: 1\n",
            "batch 39999, loss: 2.6130, risk: -0.9266, bag_size: 1\n",
            "batch 49999, loss: 1.5166, risk: -0.9231, bag_size: 1\n",
            "batch 59999, loss: 2.4746, risk: -0.9157, bag_size: 1\n",
            "batch 69999, loss: 1.4091, risk: -0.9347, bag_size: 1\n",
            "batch 79999, loss: 1.6050, risk: -0.9152, bag_size: 1\n",
            "batch 89999, loss: 1.4370, risk: -0.9314, bag_size: 1\n",
            "batch 99999, loss: 2.6518, risk: -0.9294, bag_size: 1\n",
            "batch 109999, loss: 1.4615, risk: -0.9287, bag_size: 1\n",
            "batch 119999, loss: 2.5827, risk: -0.9244, bag_size: 1\n",
            "batch 129999, loss: 1.4689, risk: -0.9279, bag_size: 1\n",
            "batch 139999, loss: 1.5402, risk: -0.9209, bag_size: 1\n",
            "batch 149999, loss: 1.6654, risk: -0.9107, bag_size: 1\n",
            "batch 159999, loss: 2.5706, risk: -0.9234, bag_size: 1\n",
            "batch 169999, loss: 2.5810, risk: -0.9242, bag_size: 1\n",
            "Epoch: 0, train_loss_surv: 1.9553, train_loss: 1.9553, train_c_index: 0.4010\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.4969, risk: -0.9251, bag_size: 1\n",
            "batch 19999, loss: 1.4429, risk: -0.9308, bag_size: 1\n",
            "batch 29999, loss: 1.4629, risk: -0.9286, bag_size: 1\n",
            "batch 39999, loss: 1.4125, risk: -0.9343, bag_size: 1\n",
            "batch 49999, loss: 1.4357, risk: -0.9316, bag_size: 1\n",
            "batch 59999, loss: 2.6065, risk: -0.9261, bag_size: 1\n",
            "batch 69999, loss: 2.4479, risk: -0.9135, bag_size: 1\n",
            "batch 79999, loss: 2.5356, risk: -0.9207, bag_size: 1\n",
            "batch 89999, loss: 2.5676, risk: -0.9232, bag_size: 1\n",
            "batch 99999, loss: 1.6490, risk: -0.9119, bag_size: 1\n",
            "batch 119999, loss: 2.5022, risk: -0.9180, bag_size: 1\n",
            "batch 129999, loss: 1.6358, risk: -0.9129, bag_size: 1\n",
            "batch 139999, loss: 1.4588, risk: -0.9290, bag_size: 1\n",
            "batch 149999, loss: 2.5238, risk: -0.9198, bag_size: 1\n",
            "batch 159999, loss: 2.4409, risk: -0.9129, bag_size: 1\n",
            "batch 169999, loss: 2.6235, risk: -0.9274, bag_size: 1\n",
            "Epoch: 1, train_loss_surv: 1.9553, train_loss: 1.9553, train_c_index: 0.4011\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.4946, risk: -0.9253, bag_size: 1\n",
            "batch 19999, loss: 1.4596, risk: -0.9288, bag_size: 1\n",
            "batch 29999, loss: 2.4834, risk: -0.9165, bag_size: 1\n",
            "batch 39999, loss: 1.5866, risk: -0.9168, bag_size: 1\n",
            "batch 49999, loss: 2.4495, risk: -0.9136, bag_size: 1\n",
            "batch 59999, loss: 2.4783, risk: -0.9160, bag_size: 1\n",
            "batch 69999, loss: 2.4517, risk: -0.9138, bag_size: 1\n",
            "batch 79999, loss: 1.5515, risk: -0.9199, bag_size: 1\n",
            "batch 89999, loss: 2.4483, risk: -0.9135, bag_size: 1\n",
            "batch 99999, loss: 1.4663, risk: -0.9282, bag_size: 1\n",
            "batch 109999, loss: 1.5005, risk: -0.9247, bag_size: 1\n",
            "batch 119999, loss: 1.4993, risk: -0.9248, bag_size: 1\n",
            "batch 129999, loss: 2.6316, risk: -0.9280, bag_size: 1\n",
            "batch 139999, loss: 1.4499, risk: -0.9300, bag_size: 1\n",
            "batch 149999, loss: 2.5628, risk: -0.9228, bag_size: 1\n",
            "batch 159999, loss: 2.5355, risk: -0.9207, bag_size: 1\n",
            "batch 169999, loss: 2.5510, risk: -0.9219, bag_size: 1\n",
            "Epoch: 2, train_loss_surv: 1.9552, train_loss: 1.9552, train_c_index: 0.4008\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.4589, risk: -0.9290, bag_size: 1\n",
            "batch 19999, loss: 2.4198, risk: -0.9110, bag_size: 1\n",
            "batch 29999, loss: 2.5818, risk: -0.9243, bag_size: 1\n",
            "batch 39999, loss: 2.4968, risk: -0.9176, bag_size: 1\n",
            "batch 49999, loss: 1.4645, risk: -0.9284, bag_size: 1\n",
            "batch 59999, loss: 2.4587, risk: -0.9144, bag_size: 1\n",
            "batch 69999, loss: 1.5881, risk: -0.9167, bag_size: 1\n",
            "batch 79999, loss: 1.4951, risk: -0.9252, bag_size: 1\n",
            "batch 89999, loss: 2.6309, risk: -0.9279, bag_size: 1\n",
            "batch 99999, loss: 1.5787, risk: -0.9175, bag_size: 1\n",
            "batch 109999, loss: 1.5002, risk: -0.9247, bag_size: 1\n",
            "batch 119999, loss: 2.4654, risk: -0.9150, bag_size: 1\n",
            "batch 129999, loss: 2.4245, risk: -0.9114, bag_size: 1\n",
            "batch 139999, loss: 2.4155, risk: -0.9106, bag_size: 1\n",
            "batch 149999, loss: 1.5280, risk: -0.9220, bag_size: 1\n",
            "batch 159999, loss: 1.4815, risk: -0.9266, bag_size: 1\n",
            "batch 169999, loss: 2.4643, risk: -0.9149, bag_size: 1\n",
            "Epoch: 3, train_loss_surv: 1.9552, train_loss: 1.9552, train_c_index: 0.4008\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 1.5343, risk: -0.9214, bag_size: 1\n",
            "batch 19999, loss: 2.4044, risk: -0.9096, bag_size: 1\n",
            "batch 29999, loss: 1.5560, risk: -0.9195, bag_size: 1\n",
            "batch 39999, loss: 2.6492, risk: -0.9292, bag_size: 1\n",
            "batch 49999, loss: 2.5923, risk: -0.9251, bag_size: 1\n",
            "batch 59999, loss: 1.4081, risk: -0.9348, bag_size: 1\n",
            "batch 69999, loss: 1.5929, risk: -0.9163, bag_size: 1\n",
            "batch 79999, loss: 1.4569, risk: -0.9292, bag_size: 1\n",
            "batch 89999, loss: 1.4584, risk: -0.9291, bag_size: 1\n",
            "batch 99999, loss: 1.4483, risk: -0.9302, bag_size: 1\n",
            "batch 109999, loss: 2.7302, risk: -0.9347, bag_size: 1\n",
            "batch 119999, loss: 1.4874, risk: -0.9260, bag_size: 1\n",
            "batch 129999, loss: 2.6373, risk: -0.9284, bag_size: 1\n",
            "batch 139999, loss: 1.4412, risk: -0.9310, bag_size: 1\n",
            "batch 149999, loss: 2.4637, risk: -0.9148, bag_size: 1\n",
            "batch 159999, loss: 2.4169, risk: -0.9108, bag_size: 1\n",
            "batch 169999, loss: 2.6358, risk: -0.9282, bag_size: 1\n",
            "Epoch: 4, train_loss_surv: 1.9551, train_loss: 1.9551, train_c_index: 0.4003\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.4759, risk: -0.9159, bag_size: 1\n",
            "batch 19999, loss: 1.4730, risk: -0.9275, bag_size: 1\n",
            "batch 29999, loss: 1.4636, risk: -0.9285, bag_size: 1\n",
            "batch 39999, loss: 2.4864, risk: -0.9167, bag_size: 1\n",
            "batch 49999, loss: 1.5080, risk: -0.9240, bag_size: 1\n",
            "batch 59999, loss: 2.6258, risk: -0.9275, bag_size: 1\n",
            "batch 69999, loss: 2.4088, risk: -0.9100, bag_size: 1\n",
            "batch 79999, loss: 1.5204, risk: -0.9227, bag_size: 1\n",
            "batch 89999, loss: 1.4639, risk: -0.9285, bag_size: 1\n",
            "batch 99999, loss: 1.4874, risk: -0.9260, bag_size: 1\n",
            "batch 109999, loss: 2.7461, risk: -0.9358, bag_size: 1\n",
            "batch 119999, loss: 2.4781, risk: -0.9160, bag_size: 1\n",
            "batch 129999, loss: 2.4025, risk: -0.9095, bag_size: 1\n",
            "batch 139999, loss: 1.4714, risk: -0.9277, bag_size: 1\n",
            "batch 149999, loss: 2.6156, risk: -0.9268, bag_size: 1\n",
            "batch 159999, loss: 2.5590, risk: -0.9226, bag_size: 1\n",
            "batch 169999, loss: 2.5134, risk: -0.9190, bag_size: 1\n",
            "Epoch: 5, train_loss_surv: 1.9550, train_loss: 1.9550, train_c_index: 0.4003\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.5658, risk: -0.9231, bag_size: 1\n",
            "batch 19999, loss: 2.6525, risk: -0.9295, bag_size: 1\n",
            "batch 29999, loss: 2.4510, risk: -0.9137, bag_size: 1\n",
            "batch 39999, loss: 1.4493, risk: -0.9300, bag_size: 1\n",
            "batch 49999, loss: 1.4113, risk: -0.9344, bag_size: 1\n",
            "batch 59999, loss: 2.7471, risk: -0.9358, bag_size: 1\n",
            "batch 69999, loss: 1.5342, risk: -0.9215, bag_size: 1\n",
            "batch 79999, loss: 1.5226, risk: -0.9225, bag_size: 1\n",
            "batch 89999, loss: 2.4308, risk: -0.9120, bag_size: 1\n",
            "batch 99999, loss: 2.5962, risk: -0.9254, bag_size: 1\n",
            "batch 109999, loss: 1.6382, risk: -0.9127, bag_size: 1\n",
            "batch 119999, loss: 2.4316, risk: -0.9120, bag_size: 1\n",
            "batch 129999, loss: 1.4407, risk: -0.9310, bag_size: 1\n",
            "batch 139999, loss: 2.4488, risk: -0.9135, bag_size: 1\n",
            "batch 149999, loss: 1.5504, risk: -0.9200, bag_size: 1\n",
            "batch 159999, loss: 1.4137, risk: -0.9341, bag_size: 1\n",
            "batch 169999, loss: 1.4241, risk: -0.9329, bag_size: 1\n",
            "Epoch: 6, train_loss_surv: 1.9550, train_loss: 1.9550, train_c_index: 0.4000\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.7116, risk: -0.9335, bag_size: 1\n",
            "batch 19999, loss: 1.4896, risk: -0.9258, bag_size: 1\n",
            "batch 29999, loss: 2.4961, risk: -0.9175, bag_size: 1\n",
            "batch 39999, loss: 2.5773, risk: -0.9239, bag_size: 1\n",
            "batch 49999, loss: 1.5445, risk: -0.9205, bag_size: 1\n",
            "batch 59999, loss: 2.7419, risk: -0.9355, bag_size: 1\n",
            "batch 69999, loss: 1.6698, risk: -0.9104, bag_size: 1\n",
            "batch 79999, loss: 2.4332, risk: -0.9122, bag_size: 1\n",
            "batch 89999, loss: 2.4682, risk: -0.9152, bag_size: 1\n",
            "batch 99999, loss: 2.6579, risk: -0.9298, bag_size: 1\n",
            "batch 109999, loss: 1.4172, risk: -0.9337, bag_size: 1\n",
            "batch 119999, loss: 1.4403, risk: -0.9311, bag_size: 1\n",
            "batch 129999, loss: 2.5006, risk: -0.9179, bag_size: 1\n",
            "batch 139999, loss: 1.3973, risk: -0.9361, bag_size: 1\n",
            "batch 149999, loss: 2.5025, risk: -0.9181, bag_size: 1\n",
            "batch 159999, loss: 1.4811, risk: -0.9267, bag_size: 1\n",
            "batch 169999, loss: 1.6385, risk: -0.9127, bag_size: 1\n",
            "Epoch: 7, train_loss_surv: 1.9549, train_loss: 1.9549, train_c_index: 0.4000\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.4019, risk: -0.9094, bag_size: 1\n",
            "batch 19999, loss: 2.5540, risk: -0.9222, bag_size: 1\n",
            "batch 29999, loss: 2.4910, risk: -0.9171, bag_size: 1\n",
            "batch 39999, loss: 1.4051, risk: -0.9352, bag_size: 1\n",
            "batch 49999, loss: 1.6381, risk: -0.9126, bag_size: 1\n",
            "batch 59999, loss: 2.4008, risk: -0.9093, bag_size: 1\n",
            "batch 69999, loss: 2.4662, risk: -0.9150, bag_size: 1\n",
            "batch 79999, loss: 1.5482, risk: -0.9202, bag_size: 1\n",
            "batch 89999, loss: 1.5966, risk: -0.9160, bag_size: 1\n",
            "batch 99999, loss: 1.4457, risk: -0.9305, bag_size: 1\n",
            "batch 109999, loss: 2.4167, risk: -0.9107, bag_size: 1\n",
            "batch 119999, loss: 1.4588, risk: -0.9290, bag_size: 1\n",
            "batch 129999, loss: 1.4858, risk: -0.9262, bag_size: 1\n",
            "batch 139999, loss: 1.4972, risk: -0.9250, bag_size: 1\n",
            "batch 149999, loss: 1.4861, risk: -0.9261, bag_size: 1\n",
            "batch 159999, loss: 2.5308, risk: -0.9203, bag_size: 1\n",
            "batch 169999, loss: 2.4846, risk: -0.9166, bag_size: 1\n",
            "Epoch: 8, train_loss_surv: 1.9549, train_loss: 1.9549, train_c_index: 0.3997\n",
            "...Train...\n",
            "\n",
            "\n",
            "batch 9999, loss: 2.5505, risk: -0.9219, bag_size: 1\n",
            "batch 19999, loss: 1.6196, risk: -0.9141, bag_size: 1\n",
            "batch 29999, loss: 1.4527, risk: -0.9297, bag_size: 1\n",
            "batch 39999, loss: 2.5738, risk: -0.9237, bag_size: 1\n",
            "batch 49999, loss: 2.5628, risk: -0.9229, bag_size: 1\n",
            "batch 59999, loss: 2.7295, risk: -0.9347, bag_size: 1\n",
            "batch 69999, loss: 1.5547, risk: -0.9196, bag_size: 1\n",
            "batch 79999, loss: 1.4807, risk: -0.9267, bag_size: 1\n",
            "batch 89999, loss: 1.5068, risk: -0.9241, bag_size: 1\n",
            "batch 99999, loss: 1.4818, risk: -0.9266, bag_size: 1\n",
            "batch 109999, loss: 1.4656, risk: -0.9283, bag_size: 1\n",
            "batch 119999, loss: 2.4369, risk: -0.9125, bag_size: 1\n",
            "batch 129999, loss: 1.4510, risk: -0.9299, bag_size: 1\n",
            "batch 139999, loss: 2.4151, risk: -0.9106, bag_size: 1\n",
            "batch 149999, loss: 2.4222, risk: -0.9112, bag_size: 1\n",
            "batch 159999, loss: 2.5719, risk: -0.9235, bag_size: 1\n",
            "batch 169999, loss: 2.6080, risk: -0.9263, bag_size: 1\n",
            "Epoch: 9, train_loss_surv: 1.9548, train_loss: 1.9548, train_c_index: 0.3998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWWhJq63v6gy",
        "outputId": "2c1330fa-3da3-4aa7-ee82-23266512e93a"
      },
      "source": [
        " model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=140450, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Dropout(p=0.25, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=4, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfwdbU0Zv7m-",
        "outputId": "c9ef1687-725c-439a-a96a-0b2e3d6cffff"
      },
      "source": [
        "''' Testing model on test patches to get final performance of model '''\n",
        "test_data = InputTest(image=dtest)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle = False, num_workers = 0, batch_size = 1)\n",
        "validate_survival(1,model,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, val_loss_surv: 1.8758, val_loss: 1.8758, val_c_index: 0.5536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaWCP3ucwANb"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/propro/resnet50_nllloss_epoch50.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLzbdnXJwDsX",
        "outputId": "333e39d3-7d2d-4933-ed6f-d9a2e24496c2"
      },
      "source": [
        "''' Checkpoint '''\n",
        "model = Attention()\n",
        "model.cuda()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/propro/resnet50_nllloss_epoch50.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv2d(3, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=140450, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Dropout(p=0.25, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=4, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaaKeFZGwRAp",
        "outputId": "2586b314-e778-4fdc-f590-fafb6fe661b3"
      },
      "source": [
        "''' Testing model on test patches to get final performance of model '''\n",
        "test_data = InputTest(image=dtest)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle = False, num_workers = 0, batch_size = 1)\n",
        "validate_survival(1,model,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, val_loss_surv: 1.8737, val_c_index: 0.5568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfUh_YIZW57t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}